
# data preperation
inspect data
- how many utterances per speaker

dataset to embeddings
- store embeddings
- visualize cohorts (visualize for speaker id to see if there is overlap (should be))
- generate labels for data using the GMM
(- see how much silence is in each datapoint and if that influences the embeddings)


split datast
- split 80 / 10 / 10 (train / dev / test) for each speaker
- results in same split for cohort and overall

run whisper on data to see if there are easier and more difficult speakers

train whisper Baseline
- hypermodel like in the paper (train on subsets of cohorts to see how it generalizes to other cohorts)
- lora adapter specific for one speaker (on one cohort and then one speaker in that cohort, expected that results for different speaker in same cohort is skewed since model saw those speakers in training)
  - train cohort -> train speaker
  - once for each cohort if Possible

train gating network
trait experts
train both jointly

evaluation
                          cohort A                cohort B                cohort C
                    speaker 1   speaker 2   speaker 3   speaker 4   speaker 5   speaker 6
#utterances             n           n           n           n           n           n

whisper                 x           x           x           x           x           x
whisper_lora_A1         x           x           x           x           x           x
whisper_lora_B3         x           x           x           x           x           x
whisper_lora_C5         x           x           x           x           x           x
whisper_hyper_AB        x           x           x           x           x           x
whisper_moe_hyper_AB    x           x           x           x           x           x

-> from the number of utterances and the score per speaker a score per cohort or a general score can be calculated